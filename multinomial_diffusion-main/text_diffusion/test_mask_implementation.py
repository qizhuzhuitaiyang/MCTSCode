#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æµ‹è¯• MASK å®ç°
éªŒè¯ä¸‰åˆ†ç±»æ‰©æ•£æ¨¡å‹ï¼ˆ0=æœªå¼€è¯, 1=å¼€è¯, 2=MASKï¼‰æ˜¯å¦æ­£ç¡®å·¥ä½œ
"""

import torch
import sys
import os
import argparse

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from datasets.dataset_mimic import MIMICDrugDataset
from model import get_model
from diffusion_utils.diffusion_multinomial import index_to_log_onehot, log_onehot_to_index

def test_mask_in_diffusion():
    """æµ‹è¯•æ‰©æ•£è¿‡ç¨‹ä¸­æ˜¯å¦å‡ºç° MASK"""
    print("=" * 70)
    print("æµ‹è¯• MASK åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„è¡¨ç°")
    print("=" * 70)
    
    # 1. åŠ è½½æ•°æ®
    print("\n[1] åŠ è½½æµ‹è¯•æ•°æ®...")
    dataset = MIMICDrugDataset(split='train', root='./datasets')
    drug_indices, condition_embedding, _ = dataset[0]
    
    print(f"   - åŸå§‹å¤„æ–¹: {drug_indices.shape}")
    print(f"   - å¼€äº† {drug_indices.sum().item()} ç§è¯")
    print(f"   - è¯ç‰©å€¼èŒƒå›´: [{drug_indices.min().item()}, {drug_indices.max().item()}]")
    print(f"   - æ¡ä»¶å‘é‡: {condition_embedding.shape}")
    
    # 2. åˆ›å»ºç®€åŒ–çš„æ¨¡å‹å‚æ•°
    print("\n[2] åˆ›å»ºæ‰©æ•£æ¨¡å‹ï¼ˆ3ç±»ï¼š0/1/2ï¼‰...")
    
    class Args:
        input_dp_rate = 0.0
        transformer_dim = 128
        transformer_heads = 4
        transformer_depth = 2
        transformer_blocks = 1
        transformer_local_heads = 2
        transformer_local_size = 95
        transformer_reversible = False
        diffusion_steps = 100
        diffusion_loss = 'vb_stochastic'
        diffusion_parametrization = 'x0'
        condition_dim = 1024
    
    args = Args()
    
    # åˆ›å»ºæ¨¡å‹
    data_shape = (190,)
    num_classes = 3  # 0=æœªå¼€è¯, 1=å¼€è¯, 2=MASK
    
    model = get_model(args, data_shape=data_shape, num_classes=num_classes)
    model.eval()
    
    print(f"   âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"   - å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   - ç±»åˆ«æ•°: {model.num_classes}")
    print(f"   - æ‰©æ•£æ­¥æ•°: {model.num_timesteps}")
    
    # 3. æµ‹è¯•å‰å‘æ‰©æ•£ï¼ˆåŠ å™ªï¼‰
    print("\n[3] æµ‹è¯•å‰å‘æ‰©æ•£è¿‡ç¨‹ï¼ˆq(x_t|x_0)ï¼‰...")
    
    # è½¬æ¢ä¸º one-hot log æ¦‚ç‡
    x = drug_indices.unsqueeze(0)  # (1, 190)
    log_x_start = index_to_log_onehot(x, num_classes)  # (1, 3, 190)
    
    print(f"   - x_0 shape: {x.shape}")
    print(f"   - log_x_start shape: {log_x_start.shape}")
    
    # åœ¨ä¸åŒæ—¶é—´æ­¥é‡‡æ ·
    timesteps_to_test = [0, 25, 50, 75, 99]
    
    print("\n   é‡‡æ ·ç»“æœ:")
    print("   " + "-" * 60)
    
    with torch.no_grad():
        for t_val in timesteps_to_test:
            t = torch.tensor([t_val])
            
            # é‡‡æ · x_t ~ q(x_t | x_0)
            x_t_sample = model.q_sample(log_x_start, t)  # (1, 3, 190) in log space
            x_t = log_onehot_to_index(x_t_sample)  # (1, 190)
            
            # ç»Ÿè®¡å„ç±»åˆ«æ•°é‡
            num_0 = (x_t == 0).sum().item()
            num_1 = (x_t == 1).sum().item()
            num_2 = (x_t == 2).sum().item()
            
            print(f"   t={t_val:3d}: æœªå¼€è¯={num_0:3d}, å¼€è¯={num_1:3d}, MASK={num_2:3d}")
    
    print("   " + "-" * 60)
    print("\n   âœ… è§‚å¯Ÿ: éšç€ t å¢å¤§ï¼ŒMASK(2) å‡ºç°é¢‘ç‡åº”è¯¥å¢åŠ ")
    print("           è¿™è¡¨æ˜æ‰©æ•£è¿‡ç¨‹æ­£åœ¨å°†ç¡®å®šçŠ¶æ€(0/1)è½¬åŒ–ä¸ºä¸ç¡®å®šçŠ¶æ€(MASK)")
    
    # 4. æµ‹è¯•åå‘æ‰©æ•£ï¼ˆå»å™ªï¼‰
    print("\n[4] æµ‹è¯•åå‘æ‰©æ•£è¿‡ç¨‹ï¼ˆp(x_{t-1}|x_t)ï¼‰...")
    
    # ä»çº¯å™ªå£°å¼€å§‹
    uniform_logits = torch.zeros((1, num_classes, 190))
    log_z = model.log_sample_categorical(uniform_logits)
    z = log_onehot_to_index(log_z)
    
    print(f"   - åˆå§‹çº¯å™ªå£° x_T:")
    num_0 = (z == 0).sum().item()
    num_1 = (z == 1).sum().item()
    num_2 = (z == 2).sum().item()
    print(f"     æœªå¼€è¯={num_0}, å¼€è¯={num_1}, MASK={num_2}")
    
    # æ¡ä»¶å‘é‡
    context = condition_embedding.unsqueeze(0)
    
    # åå‘å»å™ªå‡ æ­¥
    print("\n   åå‘å»å™ªé‡‡æ ·:")
    print("   " + "-" * 60)
    
    with torch.no_grad():
        timesteps_to_show = [99, 75, 50, 25, 0]
        for t_val in timesteps_to_show:
            if t_val < 99:
                t = torch.tensor([t_val])
                log_z = model.p_sample(log_z, t, context=context)
                z = log_onehot_to_index(log_z)
            
            num_0 = (z == 0).sum().item()
            num_1 = (z == 1).sum().item()
            num_2 = (z == 2).sum().item()
            
            print(f"   t={t_val:3d}: æœªå¼€è¯={num_0:3d}, å¼€è¯={num_1:3d}, MASK={num_2:3d}")
    
    print("   " + "-" * 60)
    print("\n   âœ… è§‚å¯Ÿ: éšç€å»å™ªè¿›è¡Œï¼ŒMASK åº”è¯¥é€æ¸å‡å°‘")
    print("           æœ€ç»ˆåªå‰©ä¸‹ 0(æœªå¼€è¯) å’Œ 1(å¼€è¯)")
    
    # 5. æµ‹è¯•æ¡ä»¶é‡‡æ ·
    print("\n[5] æµ‹è¯•æ¡ä»¶é‡‡æ ·ï¼ˆå®Œæ•´å»å™ªé“¾ï¼‰...")
    
    with torch.no_grad():
        # é‡‡æ ·3ä¸ªå¤„æ–¹
        num_samples = 3
        context_batch = condition_embedding.unsqueeze(0).repeat(num_samples, 1)
        
        print(f"   ä»çº¯å™ªå£°å¼€å§‹é‡‡æ · {num_samples} ä¸ªå¤„æ–¹...")
        
        # åˆå§‹åŒ–ä¸ºå‡åŒ€åˆ†å¸ƒ
        uniform_logits = torch.zeros((num_samples, num_classes, 190))
        log_z = model.log_sample_categorical(uniform_logits)
        
        # å®Œæ•´åå‘é“¾ï¼ˆåªæ˜¾ç¤ºéƒ¨åˆ†æ­¥éª¤ï¼‰
        for i in reversed(range(0, model.num_timesteps)):
            t = torch.full((num_samples,), i, dtype=torch.long)
            log_z = model.p_sample(log_z, t, context=context_batch)
            
            # æ¯25æ­¥æ˜¾ç¤ºä¸€æ¬¡
            if i % 25 == 0 or i == 0:
                z = log_onehot_to_index(log_z)
                num_mask = (z == 2).sum().item()
                num_prescribed = (z == 1).sum().item()
                print(f"   t={i:3d}: å«MASK={num_mask:4d}, å¼€è¯æ€»æ•°={num_prescribed:4d}")
        
        # æœ€ç»ˆç»“æœ
        final_samples = log_onehot_to_index(log_z)
        
        print("\n   æœ€ç»ˆé‡‡æ ·ç»“æœ:")
        for i in range(num_samples):
            sample = final_samples[i]
            num_0 = (sample == 0).sum().item()
            num_1 = (sample == 1).sum().item()
            num_2 = (sample == 2).sum().item()
            print(f"   æ ·æœ¬ {i+1}: æœªå¼€è¯={num_0}, å¼€è¯={num_1}, MASK={num_2}")
    
    print("\n" + "=" * 70)
    print("âœ… MASK å®ç°æµ‹è¯•å®Œæˆï¼")
    print("=" * 70)
    print("\næ€»ç»“:")
    print("  1. æ•°æ®é›†è¾“å‡º 0/1ï¼ˆç¡®å®šçŠ¶æ€ï¼‰")
    print("  2. å‰å‘æ‰©æ•£å°† 0/1 é€æ¸æ··åˆåˆ° [0,1,2]ï¼ŒMASK(2) è¡¨ç¤ºä¸ç¡®å®šçŠ¶æ€")
    print("  3. åå‘å»å™ªä» MASK æ¢å¤åˆ°ç¡®å®šçš„ 0/1")
    print("  4. æ¨¡å‹å­¦ä¹ : ç»™å®šæ¡ä»¶cå’Œå«MASKçš„x_tï¼Œé¢„æµ‹å¹²å‡€çš„x_0")
    print("\nğŸ¯ è¿™ç§è®¾è®¡ä¸‹ï¼ŒMASK æ˜¯æ‰©æ•£è¿‡ç¨‹çš„è‡ªç„¶äº§ç‰©ï¼Œ")
    print("   ä½œä¸ºå™ªå£°çš„ä¸€ç§è¡¨ç°å½¢å¼ï¼Œå¸®åŠ©æ¨¡å‹å­¦ä¹ ä¸ç¡®å®šæ€§çš„å»é™¤ã€‚")


def test_data_distribution():
    """æµ‹è¯•æ•°æ®é›†ä¸­çš„ç±»åˆ«åˆ†å¸ƒ"""
    print("\n" + "=" * 70)
    print("æ£€æŸ¥æ•°æ®é›†ä¸­çš„ç±»åˆ«åˆ†å¸ƒ")
    print("=" * 70)
    
    print("\nåŠ è½½è®­ç»ƒé›†...")
    dataset = MIMICDrugDataset(split='train', root='./datasets')
    
    print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
    
    # æ£€æŸ¥å‰10ä¸ªæ ·æœ¬
    print("\nå‰10ä¸ªæ ·æœ¬çš„ç±»åˆ«åˆ†å¸ƒ:")
    for i in range(min(10, len(dataset))):
        drug_indices, _, _ = dataset[i]
        unique_values = torch.unique(drug_indices)
        num_prescribed = (drug_indices == 1).sum().item()
        print(f"  æ ·æœ¬ {i}: å”¯ä¸€å€¼={unique_values.tolist()}, å¼€è¯æ•°={num_prescribed}")
    
    print("\nâœ… ç¡®è®¤: æ•°æ®é›†åªè¾“å‡º 0/1ï¼ŒMASK(2) ç”±æ‰©æ•£è¿‡ç¨‹åŠ¨æ€ç”Ÿæˆ")


if __name__ == "__main__":
    # æµ‹è¯•MASKåœ¨æ‰©æ•£ä¸­çš„è¡¨ç°
    test_mask_in_diffusion()
    
    # æµ‹è¯•æ•°æ®åˆ†å¸ƒ
    test_data_distribution()

