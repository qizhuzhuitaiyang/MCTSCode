# 从零开始：MIMIC-IV 药物推荐数据集预处理指南

## 📋 项目概述

本项目使用 MIMIC-IV 数据集构建药物推荐的扩散模型，核心思路：
- **输入**：患者条件向量（1024 维）
- **输出**：药物组合向量（189 维 multi-hot）
- **模型**：基于 Multinomial Diffusion 的 text_diffusion

---

## 🔧 前置准备

### 1. 环境要求

- Python 3.8+
- PyTorch 1.10+
- CUDA（可选，用于 GPU 加速）

### 2. 创建 Conda 环境

```bash
conda create -n llamafactory python=3.8
conda activate llamafactory
```

### 3. 安装依赖

```bash
pip install torch torchvision
pip install pandas numpy
pip install pyhealth  # MIMIC-IV 数据加载
pip install tqdm
```

### 4. 确认 MIMIC-IV 数据集路径

确保你有权访问 MIMIC-IV 数据集，路径为：
```
/mnt/share/Zhiwen/mimic-iv-2.2/hosp/
```

该目录应包含：
- `patients.csv.gz` - 患者基本信息
- `admissions.csv.gz` - 入院信息
- `diagnoses_icd.csv.gz` - 诊断编码
- `procedures_icd.csv.gz` - 手术编码
- `prescriptions.csv.gz` - 处方信息

---

## 🚀 完整执行流程（按顺序运行）

### 步骤 1: 构建词表（约 3-5 分钟）

**目的**：统计并构建药物、诊断、手术的词表

```bash
cd /home/zhuwei/zhangjian/MCTScode/mct_diffusion2/multinomial_diffusion-main/text_diffusion/datasets

conda activate llamafactory

python build_vocabularies.py \
    --mimic_root /mnt/share/Zhiwen/mimic-iv-2.2/hosp \
    --output_dir ./mimic_drugs \
    --top_k_diagnosis 400 \
    --top_k_procedure 150
```

**生成的文件**：
```
datasets/mimic_drugs/
├── drug_vocab.json                    # 189 个 ATC Level 3 药物
├── diagnosis_vocab_aggregated.json    # 401 个诊断类目（覆盖率 86.39%）
├── procedure_vocab_aggregated.json    # 151 个手术类目（覆盖率 99.99%）
└── vocab_stats.json                   # 统计信息
```

**预期输出示例**：
```
============================================================
构建 MIMIC-IV 词表
============================================================
1. 加载 MIMIC-IV 数据集...
2. 应用药物推荐任务...
总样本数: 147,393
训练集样本数: 103,175
3. 统计诊断编码频率...
唯一诊断类目数: 2,877
诊断记录总数: 5,049,090
4. 统计手术编码频率...
唯一手术类目数: 165
手术记录总数: 904,996
5. 统计药物频率...
唯一药物数 (ATC-L3): 189
药物记录总数: 2,312,505
6. 构建词表...
诊断词表: 保留 top-400 类目，覆盖率: 86.39%
手术词表: 保留 top-150 类目，覆盖率: 99.99%
药物词表: 189 个 ATC-L3 药物
7. 保存词表...
✓ 诊断词表已保存: ./mimic_drugs/diagnosis_vocab_aggregated.json
✓ 手术词表已保存: ./mimic_drugs/procedure_vocab_aggregated.json
✓ 药物词表已保存: ./mimic_drugs/drug_vocab.json
✓ 统计信息已保存: ./mimic_drugs/vocab_stats.json
```

---

### 步骤 2: 测试词表和 Embedding（可选，约 1 分钟）

**目的**：验证词表构建和患者条件向量构建是否正常

```bash
cd /home/zhuwei/zhangjian/MCTScode/mct_diffusion2/multinomial_diffusion-main/text_diffusion

conda activate llamafactory

# 运行完整测试
bash quick_start_embedding_v2.sh
```

或者单独测试：

```bash
# 测试 ICD 代码聚合
python -c "
from datasets.icd_aggregation import aggregate_diagnosis_code, aggregate_procedure_code
print('诊断 5723 ->', aggregate_diagnosis_code('5723'))
print('诊断 E1165 ->', aggregate_diagnosis_code('E1165'))
print('手术 5491 ->', aggregate_procedure_code('5491'))
"

# 测试 Elixhauser 提取
python datasets/elixhauser.py

# 测试 PatientEmbeddingV2
python test_embedding_v2_complete.py
```

**预期输出**：
```
============================================================
测试 1: ICD 代码聚合
============================================================
✓ 诊断 5723 → 572
✓ 诊断 E1165 → E11
✓ 手术 5491 → 54
...（所有测试通过）
```

---

### 步骤 3: 预处理数据集（约 20-40 分钟）⚠️ **最耗时**

**目的**：
1. 加载 MIMIC-IV 原始数据
2. 构建患者条件向量（1024 维）
3. 构建药物组合向量（189 维）
4. 划分训练/验证/测试集（70%/15%/15%）
5. 保存预处理后的数据

#### 方法 A: 使用 Python 脚本（推荐）

创建脚本 `preprocess_all_splits.py`：

```python
#!/usr/bin/env python3
"""
预处理所有数据集 split（train, valid, test）
"""
import os
import sys
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from datasets.dataset_mimic import MIMICDrugDataset

def preprocess_all():
    print("=" * 80)
    print("开始预处理 MIMIC-IV 数据集")
    print("=" * 80)
    
    splits = ['train', 'valid', 'test']
    
    for split in splits:
        print(f"\n{'='*80}")
        print(f"处理 {split.upper()} 集")
        print(f"{'='*80}")
        
        try:
            dataset = MIMICDrugDataset(
                root='./datasets',
                split=split,
                max_drugs=190,
                condition_dim=1024,
                mimic_root='/mnt/share/Zhiwen/mimic-iv-2.2/hosp'
            )
            
            print(f"✓ {split} 集预处理完成")
            print(f"  - 样本数: {len(dataset)}")
            print(f"  - 药物词表大小: {len(dataset.drug_vocab)}")
            print(f"  - 条件向量维度: {dataset.condition_dim}")
            
        except Exception as e:
            print(f"✗ {split} 集预处理失败: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    print("\n" + "=" * 80)
    print("所有数据集预处理完成！")
    print("=" * 80)
    return True

if __name__ == "__main__":
    success = preprocess_all()
    sys.exit(0 if success else 1)
```

运行：

```bash
cd /home/zhuwei/zhangjian/MCTScode/mct_diffusion2/multinomial_diffusion-main/text_diffusion

conda activate llamafactory

python preprocess_all_splits.py
```

#### 方法 B: 使用 Python 交互式（了解细节）

```python
import sys
sys.path.insert(0, '/home/zhuwei/zhangjian/MCTScode/mct_diffusion2/multinomial_diffusion-main/text_diffusion')

from datasets.dataset_mimic import MIMICDrugDataset

# 预处理训练集（约 15-25 分钟）
print("处理训练集...")
train_dataset = MIMICDrugDataset(
    root='./datasets',
    split='train',
    max_drugs=190,
    condition_dim=1024,
    mimic_root='/mnt/share/Zhiwen/mimic-iv-2.2/hosp'
)
print(f"✓ 训练集: {len(train_dataset)} 个样本")

# 预处理验证集（约 3-5 分钟）
print("处理验证集...")
valid_dataset = MIMICDrugDataset(
    root='./datasets',
    split='valid',
    max_drugs=190,
    condition_dim=1024,
    mimic_root='/mnt/share/Zhiwen/mimic-iv-2.2/hosp'
)
print(f"✓ 验证集: {len(valid_dataset)} 个样本")

# 预处理测试集（约 3-5 分钟）
print("处理测试集...")
test_dataset = MIMICDrugDataset(
    root='./datasets',
    split='test',
    max_drugs=190,
    condition_dim=1024,
    mimic_root='/mnt/share/Zhiwen/mimic-iv-2.2/hosp'
)
print(f"✓ 测试集: {len(test_dataset)} 个样本")
```

**生成的文件**：
```
datasets/mimic_drugs/
├── processed_train.pt      # 训练集药物向量 (103,175 × 190)
├── conditions_train.pt     # 训练集条件向量 (103,175 × 1024)
├── metadata_train.pt       # 训练集元数据 (subject_id, hadm_id)
├── processed_valid.pt      # 验证集药物向量 (~22,109 × 190)
├── conditions_valid.pt     # 验证集条件向量 (~22,109 × 1024)
├── metadata_valid.pt       # 验证集元数据
├── processed_test.pt       # 测试集药物向量 (~22,109 × 190)
├── conditions_test.pt      # 测试集条件向量 (~22,109 × 1024)
└── metadata_test.pt        # 测试集元数据
```

**预期输出示例**：
```
============================================================
处理 TRAIN 集
============================================================
Loading patients and admissions data from /mnt/share/Zhiwen/mimic-iv-2.2/hosp...
✓ Loaded 299712 patients and 431231 admissions
Preprocessing train data...
Processing 103175 samples for train split
Processing sample 0/103175
Processing sample 1000/103175
...
Processing sample 103000/103175
Saved 103175 samples to ./datasets/mimic_drugs/processed_train.pt
✓ train 集预处理完成
  - 样本数: 103175
  - 药物词表大小: 189
  - 条件向量维度: 1024
```

---

### 步骤 4: 验证数据集（约 1 分钟）

**目的**：确认数据集预处理正确，可以正常加载

```bash
cd /home/zhuwei/zhangjian/MCTScode/mct_diffusion2/multinomial_diffusion-main/text_diffusion

conda activate llamafactory

python test_dataset_mimic_updated.py
```

**预期输出**：
```
╔==============================================================================╗
║                    测试更新后的 MIMICDrugDataset                            ║
╚==============================================================================╝

============================================================
测试 1: 创建训练集
============================================================
✓ 训练集创建成功
  - 样本数: 103175
  - 药物词表大小: 189
  - 条件向量维度: 1024

============================================================
测试 2: 获取数据样本
============================================================
✓ 样本获取成功
  - drug_indices shape: torch.Size([190])
  - drug_indices dtype: torch.int64
  - drug_indices unique values: tensor([0, 1])
  - condition_embedding shape: torch.Size([1024])
  - condition_embedding dtype: torch.float32
  - condition_embedding range: [0.0000, 1.0000]
  - max_drugs: 190
✓ 所有维度验证通过

============================================================
测试 3: 批量加载
============================================================
✓ 批量加载成功
  - drug_batch shape: torch.Size([4, 190])
  - condition_batch shape: torch.Size([4, 1024])
  - max_drugs_batch: tensor([190, 190, 190, 190])
✓ 批量维度验证通过

============================================================
测试 4: Metadata 存储
============================================================
✓ Metadata 加载成功
  - 样本数: 103175
  - 前 3 个样本:
    [0] subject_id=10000032, hadm_id=29079034
    [1] subject_id=10000032, hadm_id=26222711
    [2] subject_id=10000048, hadm_id=22595853

============================================================
测试 5: 词表一致性
============================================================
✓ build_vocabularies.py 生成的词表: 189 个药物
✓ dataset_mimic.py 使用的词表: 189 个药物
✓ 两个词表完全一致

============================================================
测试完成!
============================================================
```

---

## 📊 数据集统计信息

完成预处理后，你将得到：

| 指标 | 数值 |
|------|------|
| **总样本数** | 147,393 |
| **训练集** | 103,175 (70%) |
| **验证集** | ~22,109 (15%) |
| **测试集** | ~22,109 (15%) |
| **药物词表大小** | 189 个 ATC Level 3 药物 |
| **诊断类目数** | 401 个（覆盖率 86.39%） |
| **手术类目数** | 151 个（覆盖率 99.99%） |
| **药物向量维度** | 190（189 个药物 + 1 个余量） |
| **条件向量维度** | 1024 |
| **磁盘占用** | ~500 MB - 1 GB |

---

## 📁 最终文件结构

```
text_diffusion/
├── datasets/
│   ├── mimic_drugs/                    # 所有预处理数据
│   │   ├── drug_vocab.json             # 药物词表
│   │   ├── diagnosis_vocab_aggregated.json
│   │   ├── procedure_vocab_aggregated.json
│   │   ├── vocab_stats.json
│   │   ├── processed_train.pt          # 训练集数据
│   │   ├── conditions_train.pt
│   │   ├── metadata_train.pt
│   │   ├── processed_valid.pt          # 验证集数据
│   │   ├── conditions_valid.pt
│   │   ├── metadata_valid.pt
│   │   ├── processed_test.pt           # 测试集数据
│   │   ├── conditions_test.pt
│   │   └── metadata_test.pt
│   ├── dataset_mimic.py                # 数据集类
│   ├── patient_embedding_v2.py         # 患者条件向量构建
│   ├── icd_aggregation.py              # ICD 代码聚合
│   ├── elixhauser.py                   # Elixhauser 并存病提取
│   └── build_vocabularies.py           # 词表构建脚本
├── test_dataset_mimic_updated.py       # 测试脚本
├── test_embedding_v2_complete.py       # Embedding 测试
├── quick_start_embedding_v2.sh         # 快速测试脚本
└── 从零开始使用指南.md                  # 本文件
```

---

## 🎯 数据格式说明

### 药物向量（D）

- **形状**：`(190,)`
- **类型**：`LongTensor`
- **取值**：`{0, 1}`
  - `0` = 未开此药
  - `1` = 开了此药
- **示例**：`[0, 1, 0, 0, 1, 1, 0, ...]`（189 个药物位置）

### 条件向量（c）

- **形状**：`(1024,)`
- **类型**：`FloatTensor`
- **取值**：归一化后的浮点数（通常在 0-1 之间）
- **组成**：

| 特征块 | 维度 | 编码方式 |
|--------|------|----------|
| 诊断编码 | 400 | Multi-hot（ICD 3位类目） |
| 手术编码 | 150 | Multi-hot（ICD 2位大类） |
| Elixhauser 并存病 | 31 | Binary（0/1） |
| 历史用药 | 190 | Multi-hot（ATC-L3） |
| 患者人口学特征 | 253 | 连续 + One-hot |
| **总计** | **1024** | - |

---

## 🔍 使用数据集的代码示例

### 单个样本

```python
from datasets.dataset_mimic import MIMICDrugDataset

# 加载训练集
train_dataset = MIMICDrugDataset(
    root='./datasets',
    split='train',
    max_drugs=190,
    condition_dim=1024
)

# 获取第一个样本
drug_indices, condition_embedding, max_drugs = train_dataset[0]

print(f"药物向量: {drug_indices.shape}")        # (190,)
print(f"开了几个药: {drug_indices.sum()}")      # 例如: 12
print(f"条件向量: {condition_embedding.shape}") # (1024,)
```

### 批量加载

```python
from torch.utils.data import DataLoader

# 创建 DataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4
)

# 迭代批次
for drug_batch, condition_batch, max_drugs_batch in train_loader:
    # drug_batch: (32, 190)
    # condition_batch: (32, 1024)
    
    # 这里可以喂给扩散模型训练
    # loss = model(drug_batch, context=condition_batch)
    # loss.backward()
    # optimizer.step()
    
    break  # 只看第一个 batch
```

---

## ⚠️ 常见问题

### Q1: 报错 "No such file or directory: diagnosis_vocab_aggregated.json"

**原因**：没有先运行步骤 1 构建词表。

**解决**：先运行 `build_vocabularies.py`。

---

### Q2: 预处理很慢，可以加速吗？

**原因**：需要处理 10 万+ 样本，每个样本需要构建 1024 维向量。

**解决**：
- 使用 SSD 硬盘（而非机械硬盘）
- 确保有足够内存（建议 16GB+）
- 首次预处理后会保存到磁盘，下次直接加载很快

---

### Q3: 报错 "Missing subject_id or hadm_id for sample"

**原因**：极少数样本可能缺少 ID 信息。

**解决**：代码会自动跳过这些样本，属于正常现象。

---

### Q4: 内存不足怎么办？

**解决**：
- 减少 DataLoader 的 `num_workers`
- 减少 `batch_size`
- 增加系统 swap 空间

---

### Q5: 想重新预处理怎么办？

**解决**：删除旧文件后重新运行步骤 3

```bash
cd /home/zhuwei/zhangjian/MCTScode/mct_diffusion2/multinomial_diffusion-main/text_diffusion

rm -f datasets/mimic_drugs/processed_*.pt
rm -f datasets/mimic_drugs/conditions_*.pt
rm -f datasets/mimic_drugs/metadata_*.pt

# 然后重新运行预处理
python preprocess_all_splits.py
```

---

## ✅ 完成检查清单

预处理完成后，确认以下文件都存在：

```bash
cd /home/zhuwei/zhangjian/MCTScode/mct_diffusion2/multinomial_diffusion-main/text_diffusion/datasets/mimic_drugs

ls -lh
```

应该看到：

- [x] `drug_vocab.json` (~10 KB)
- [x] `diagnosis_vocab_aggregated.json` (~20 KB)
- [x] `procedure_vocab_aggregated.json` (~5 KB)
- [x] `vocab_stats.json` (~2 KB)
- [x] `processed_train.pt` (~80 MB)
- [x] `conditions_train.pt` (~400 MB)
- [x] `metadata_train.pt` (~10 MB)
- [x] `processed_valid.pt` (~20 MB)
- [x] `conditions_valid.pt` (~90 MB)
- [x] `metadata_valid.pt` (~2 MB)
- [x] `processed_test.pt` (~20 MB)
- [x] `conditions_test.pt` (~90 MB)
- [x] `metadata_test.pt` (~2 MB)

---

## 📞 需要帮助？

如果遇到问题，可以：

1. **查看日志输出**：每个步骤都会打印详细信息
2. **运行测试脚本**：`test_dataset_mimic_updated.py`
3. **检查文件完整性**：确认所有文件都生成了

---

## 🎉 完成！

如果所有步骤都成功运行，你现在已经完成了数据预处理！

下一步可以：
- 训练扩散模型
- 构建 MCTS 奖励函数
- 进行药物推荐预测

---

**文档版本**: v1.0  
**最后更新**: 2025-10-21  
**作者**: Your Team

