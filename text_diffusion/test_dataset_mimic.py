#!/usr/bin/env python
# coding: utf-8

"""
æµ‹è¯•MIMICæ•°æ®é›†å¹¶ç”Ÿæˆä¿å­˜çš„æ–‡ä»¶
"""

import sys
import os

# æ·»åŠ è·¯å¾„
sys.path.append('/home/zhuwei/zhangjian/MCTScode/mct_diffusion2/multinomial_diffusion-main/text_diffusion')

from datasets.dataset_mimic import MIMICDrugDataset


def test_dataset_creation():
    """æµ‹è¯•æ•°æ®é›†åˆ›å»ºå¹¶ç”Ÿæˆæ–‡ä»¶"""
    print("=" * 60)
    print("æµ‹è¯•MIMICæ•°æ®é›†åˆ›å»º")
    print("=" * 60)
    
    try:
        # åˆ›å»ºè®­ç»ƒæ•°æ®é›†ï¼ˆè¿™ä¼šè§¦å‘æ•°æ®é¢„å¤„ç†ï¼‰
        print("åˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
        train_dataset = MIMICDrugDataset(
            root='./datasets',
            split='train',
            max_drugs=190,
            condition_dim=512
        )
        
        print(f"âœ“ è®­ç»ƒæ•°æ®é›†åˆ›å»ºæˆåŠŸ")
        print(f"  - æ•°æ®é›†å¤§å°: {len(train_dataset)}")
        print(f"  - è¯ç‰©å‘é‡å½¢çŠ¶: {train_dataset[0][0].shape}")
        print(f"  - æ¡ä»¶embeddingå½¢çŠ¶: {train_dataset[0][1].shape}")
        print(f"  - åºåˆ—é•¿åº¦: {train_dataset[0][2]}")
        
        # åˆ›å»ºéªŒè¯æ•°æ®é›†
        print("\nåˆ›å»ºéªŒè¯æ•°æ®é›†...")
        valid_dataset = MIMICDrugDataset(
            root='./datasets',
            split='valid',
            max_drugs=190,
            condition_dim=512
        )
        
        print(f"âœ“ éªŒè¯æ•°æ®é›†åˆ›å»ºæˆåŠŸ")
        print(f"  - æ•°æ®é›†å¤§å°: {len(valid_dataset)}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        print("\nåˆ›å»ºæµ‹è¯•æ•°æ®é›†...")
        test_dataset = MIMICDrugDataset(
            root='./datasets',
            split='test',
            max_drugs=190,
            condition_dim=512
        )
        
        print(f"âœ“ æµ‹è¯•æ•°æ®é›†åˆ›å»ºæˆåŠŸ")
        print(f"  - æ•°æ®é›†å¤§å°: {len(test_dataset)}")
        
        # æ£€æŸ¥ä¿å­˜çš„æ–‡ä»¶
        print("\næ£€æŸ¥ä¿å­˜çš„æ–‡ä»¶...")
        data_dir = './datasets/mimic_drugs'
        
        files_to_check = [
            'drug_vocab.json',
            'diagnosis_vocab.json',
            'procedure_vocab.json',
            'processed_train.pt',
            'processed_valid.pt',
            'processed_test.pt',
            'conditions_train.pt',
            'conditions_valid.pt',
            'conditions_test.pt'
        ]
        
        for filename in files_to_check:
            filepath = os.path.join(data_dir, filename)
            if os.path.exists(filepath):
                file_size = os.path.getsize(filepath)
                print(f"âœ“ {filename}: {file_size:,} bytes")
            else:
                print(f"âœ— {filename}: ä¸å­˜åœ¨")
        
        print("\n" + "=" * 60)
        print("æ•°æ®é›†åˆ›å»ºå®Œæˆ!")
        print("=" * 60)
        
        return True
        
    except Exception as e:
        print(f"âœ— æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•MIMICæ•°æ®é›†åˆ›å»º")
    
    success = test_dataset_creation()
    
    if success:
        print("\nğŸ‰ æ•°æ®é›†åˆ›å»ºæˆåŠŸ! æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜ã€‚")
    else:
        print("\nâŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")


if __name__ == "__main__":
    main()
